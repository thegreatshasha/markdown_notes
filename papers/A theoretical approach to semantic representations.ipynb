{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A theoretical approach to semantic representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do low dimensional word vectors exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative model for knowledge\n",
    "\n",
    "Spatially isotropic word vectors\n",
    "\n",
    "$$ P(w\\ is\\ output) \\propto e^{v_{w}.c_{t}} $$\n",
    "$$ KL(w,w') = v_{w}.v_{w'}/d + O(\\epsilon) $$\n",
    "$$ log(P|w|) = \\|v_{w}\\|^2/d - log Z + \\epsilon $$\n",
    "\n",
    "* Norm decides frequency\n",
    "* Spatial ordering decides cooccurence\n",
    "* Existing embedding methods can also be derived from this framework\n",
    "\n",
    "### Weighted svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do relations correspond to directions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Theoretical support for the following\n",
    "$$ v_a - v_b = \\mu_{R} + noise   \\ \\forall(A,B) \\in R $$\n",
    "\n",
    "$$ log(\\frac {P(x \\mid a)} {P(x \\mid b)}) =  \\mu_{R}(X) + noise $$\n",
    "\n",
    "* For arora's model, noise is really small and $ \\mu_{R} $ can always be found\n",
    "* Proof uses linear regression\n",
    "* Uses the fact that word vectors are isotropically distributed\n",
    "* Log of probabilities are related by linear embeddings which is pretty interesting.\n",
    "* Log linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does $ v_{tie} $ contain the various meanings of $tie$\n",
    "* Tie can have differnt meanings: tie someone up, tie to weat\n",
    "* Take words with unrelated meanings like this, call them $tie_{1}, tie_2{}$\n",
    "* $ tie_{1} $ and $ tie_{2} $ correspond to different discourses\n",
    "* Take two unrelated words $ w_{i}, w_{j} $\n",
    "* Assume $ w_1 $ is 100 times more likely than $ w_2 $\n",
    "* Compute embedding of the resultant word in our model\n",
    "* The embedding is like $ 0.9v_{w_{1}} + 0.2v_{w_{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need a sparse representation of $ R_d $\n",
    "* Take 2000 random discourse directions $A_{1}$, $A_{2}$ ... $A_{2000}$\n",
    "* Each vector in $R_{d}$ represented as a combination of at most 5 of them (sparsity) plus noise\n",
    "* They were able to find a sparse basis.\n",
    "* Example: ![sp](sparse_coding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "navigate_menu": false,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
